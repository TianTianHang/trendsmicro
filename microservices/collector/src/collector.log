2025-02-23 14:59:49 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:39:29 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:57:38 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:58:45 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:59:20 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 16:59:20 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 08:59:20 UTC], next run at: 2025-02-23 08:59:20 UTC)" (scheduled at 2025-02-23 08:59:20.114674+00:00)
2025-02-23 16:59:20 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 16:59:20 - fastapi - INFO - 开始采集数据
2025-02-23 16:59:20 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 08:59:20 UTC], next run at: 2025-02-23 08:59:20 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 16, in execute_task
    get_interest_over_time(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 125, in get_interest_over_time
    timeframe_start = datetime.strptime(start, "%Y-%m-%d %H:%M:%S")
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 349, in _strptime
    raise ValueError("time data %r does not match format %r" %
ValueError: time data '2025-01-01' does not match format '%Y-%m-%d %H:%M:%S'
2025-02-23 17:01:55 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:02:07 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:02:07 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:02:07 UTC], next run at: 2025-02-23 09:02:07 UTC)" (scheduled at 2025-02-23 09:02:07.959934+00:00)
2025-02-23 17:02:08 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:02:08 - fastapi - INFO - 开始采集数据
2025-02-23 17:02:08 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 09:02:07 UTC], next run at: 2025-02-23 09:02:07 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 16, in execute_task
    get_interest_over_time(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 125, in get_interest_over_time
    timeframe_start = datetime.strptime(start, "%Y-%m-%d %H:%M:%S")
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 349, in _strptime
    raise ValueError("time data %r does not match format %r" %
ValueError: time data '2025-01-01' does not match format '%Y-%m-%d %H:%M:%S'
2025-02-23 17:04:04 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:04:12 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:04:12 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:04:12 UTC], next run at: 2025-02-23 09:04:12 UTC)" (scheduled at 2025-02-23 09:04:12.533295+00:00)
2025-02-23 17:04:12 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 17:04:12 - fastapi - INFO - 开始采集数据
2025-02-23 17:04:23 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:04:23 - fastapi - ERROR - Error: strptime() argument 1 must be str, not Timestamp
2025-02-23 17:04:23 - fastapi - INFO - 采集数据结束
2025-02-23 17:04:23 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:04:12 UTC], next run at: 2025-02-23 09:04:12 UTC)" executed successfully
2025-02-23 17:05:51 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:05:51 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:05:51 UTC], next run at: 2025-02-23 09:05:51 UTC)" (scheduled at 2025-02-23 09:05:51.325064+00:00)
2025-02-23 17:05:51 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:05:51 - fastapi - INFO - 开始采集数据
2025-02-23 17:05:53 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:12:23 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:12:59 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:12:59 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:12:59 UTC], next run at: 2025-02-23 09:12:59 UTC)" (scheduled at 2025-02-23 09:12:59.062125+00:00)
2025-02-23 17:12:59 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:12:59 - fastapi - INFO - 开始采集数据
2025-02-23 17:13:23 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 17:14:25 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:14:25 - fastapi - ERROR - Error: 'isPartial'
2025-02-23 17:14:25 - fastapi - INFO - 采集数据结束
2025-02-23 17:14:25 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:12:59 UTC], next run at: 2025-02-23 09:12:59 UTC)" executed successfully
2025-02-23 17:15:21 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:15:21 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:15:21 UTC], next run at: 2025-02-23 09:15:21 UTC)" (scheduled at 2025-02-23 09:15:21.270976+00:00)
2025-02-23 17:15:21 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:15:21 - fastapi - INFO - 开始采集数据
2025-02-23 17:15:22 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:17:24 - fastapi - ERROR - Error: 'isPartial'
2025-02-23 17:17:24 - fastapi - INFO - 采集数据结束
2025-02-23 17:17:24 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:15:21 UTC], next run at: 2025-02-23 09:15:21 UTC)" executed successfully
2025-02-23 17:17:48 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:17:48 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:17:48 UTC], next run at: 2025-02-23 09:17:48 UTC)" (scheduled at 2025-02-23 09:17:48.749147+00:00)
2025-02-23 17:17:48 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:17:48 - fastapi - INFO - 开始采集数据
2025-02-23 17:17:50 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:17:53 - fastapi - ERROR - Error: 'isPartial'
2025-02-23 17:17:53 - fastapi - INFO - 采集数据结束
2025-02-23 17:17:53 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:17:48 UTC], next run at: 2025-02-23 09:17:48 UTC)" executed successfully
2025-02-23 17:18:16 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:18:32 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:18:32 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:18:32 UTC], next run at: 2025-02-23 09:18:32 UTC)" (scheduled at 2025-02-23 09:18:32.520298+00:00)
2025-02-23 17:18:32 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:18:32 - fastapi - INFO - 开始采集数据
2025-02-23 17:18:43 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:18:43 - fastapi - ERROR - Error: (builtins.TypeError) SQLite DateTime type only accepts Python datetime and date objects as input.
[SQL: INSERT INTO time_interest (keywords, geo_code, time, "values", is_partial) VALUES (?, ?, ?, ?, ?) RETURNING id]
[parameters: [{'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-01 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-02 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-03 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-04 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-05 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-06 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(26), np.int64(14)], 'time': '2025-01-07 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(32), np.int64(14)], 'time': '2025-01-08 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(12), np.int64(6)], 'time': '2025-01-09 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(6), np.int64(2)], 'time': '2025-01-10 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(4), np.int64(1)], 'time': '2025-01-11 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(1)], 'time': '2025-01-12 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(1)], 'time': '2025-01-13 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(1)], 'time': '2025-01-14 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(1)], 'time': '2025-01-15 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-16 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(0)], 'time': '2025-01-17 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(0)], 'time': '2025-01-18 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(0)], 'time': '2025-01-19 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(33), np.int64(16)], 'time': '2025-01-20 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(46), np.int64(36)], 'time': '2025-01-21 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(23), np.int64(19)], 'time': '2025-01-22 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(13), np.int64(11)], 'time': '2025-01-23 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(10), np.int64(8)], 'time': '2025-01-24 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(15), np.int64(15)], 'time': '2025-01-25 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(12), np.int64(10)], 'time': '2025-01-26 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(8), np.int64(7)], 'time': '2025-01-27 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(28), np.int64(24)], 'time': '2025-01-28 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(17), np.int64(15)], 'time': '2025-01-29 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(10), np.int64(8)], 'time': '2025-01-30 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(8), np.int64(7)], 'time': '2025-01-31 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(6), np.int64(6)], 'time': '2025-02-01 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(6), np.int64(6)], 'time': '2025-02-02 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(5), np.int64(5)], 'time': '2025-02-03 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(4), np.int64(4)], 'time': '2025-02-04 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(4), np.int64(3)], 'time': '2025-02-05 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(3)], 'time': '2025-02-06 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(3)], 'time': '2025-02-07 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(3)], 'time': '2025-02-08 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(5), np.int64(8)], 'time': '2025-02-09 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(18), np.int64(35)], 'time': '2025-02-10 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(75), np.int64(69)], 'time': '2025-02-11 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(100), np.int64(43)], 'time': '2025-02-12 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(82), np.int64(29)], 'time': '2025-02-13 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(46), np.int64(16)], 'time': '2025-02-14 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(30), np.int64(12)], 'time': '2025-02-15 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(23), np.int64(9)], 'time': '2025-02-16 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(16), np.int64(7)], 'time': '2025-02-17 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(16), np.int64(7)], 'time': '2025-02-18 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}]]
2025-02-23 17:18:43 - fastapi - INFO - 采集数据结束
2025-02-23 17:18:43 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:18:32 UTC], next run at: 2025-02-23 09:18:32 UTC)" executed successfully
2025-02-23 17:21:58 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:22:06 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:22:06 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:22:06 UTC], next run at: 2025-02-23 09:22:06 UTC)" (scheduled at 2025-02-23 09:22:06.355900+00:00)
2025-02-23 17:22:06 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 17:22:06 - fastapi - INFO - 开始采集数据
2025-02-23 17:22:06 - fastapi - ERROR - 数据已存在或冲突: (sqlite3.IntegrityError) UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end
[SQL: INSERT INTO request_history (job_type, keywords, geo_code, timeframe_start, timeframe_end, created_at, status) VALUES (?, ?, ?, ?, ?, ?, ?)]
[parameters: ('time', '["Gulf of Mexico", "Gulf of America"]', '', '2025-01-01', '2025-02-18', '2025-02-23', 'created')]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-02-23 17:22:06 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 09:22:06 UTC], next run at: 2025-02-23 09:22:06 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\default.py", line 942, in do_execute
    cursor.execute(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 16, in execute_task
    get_interest_over_time(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 150, in get_interest_over_time
    db.commit()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 2032, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 1313, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 1288, in _prepare_impl
    self.session.flush()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4353, in flush
    self._flush(objects)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4488, in _flush
    with util.safe_reraise():
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4449, in _flush
    flush_context.execute()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\unitofwork.py", line 466, in execute
    rec.execute(self)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1416, in execute
    return meth(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\sql\elements.py", line 515, in _execute_on_connection
    return connection._execute_clauseelement(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1638, in _execute_clauseelement
    ret = self._execute_context(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1843, in _execute_context
    return self._exec_single_context(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1983, in _exec_single_context
    self._handle_dbapi_exception(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 2352, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\default.py", line 942, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end
[SQL: INSERT INTO request_history (job_type, keywords, geo_code, timeframe_start, timeframe_end, created_at, status) VALUES (?, ?, ?, ?, ?, ?, ?)]
[parameters: ('time', '["Gulf of Mexico", "Gulf of America"]', '', '2025-01-01', '2025-02-18', '2025-02-23', 'created')]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-02-23 17:22:42 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:22:42 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:22:42 UTC], next run at: 2025-02-23 09:22:42 UTC)" (scheduled at 2025-02-23 09:22:42.053996+00:00)
2025-02-23 17:22:42 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:22:42 - fastapi - INFO - 开始采集数据
2025-02-23 17:22:45 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:24:20 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:24:24 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:24:24 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:24:24 UTC], next run at: 2025-02-23 09:24:24 UTC)" (scheduled at 2025-02-23 09:24:24.615557+00:00)
2025-02-23 17:24:24 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:24:24 - fastapi - INFO - 开始采集数据
2025-02-23 17:24:34 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:24:34 - fastapi - INFO - 采集数据结束
2025-02-23 17:24:34 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:24:24 UTC], next run at: 2025-02-23 09:24:24 UTC)" executed successfully
2025-02-23 18:05:48 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 18:05:48 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 10:05:48 UTC], next run at: 2025-02-23 10:05:48 UTC)" (scheduled at 2025-02-23 10:05:48.284175+00:00)
2025-02-23 18:05:48 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 18:05:48 - fastapi - INFO - 开始采集数据
2025-02-23 18:06:05 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:07:21 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:08:38 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:09:38 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-23 18:09:38 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 10:05:48 UTC], next run at: 2025-02-23 10:05:48 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 14, in execute_task
    get_interest_by_region(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 86, in get_interest_by_region
    region_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-23 18:13:06 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 18:13:06 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 10:13:06 UTC], next run at: 2025-02-23 10:13:06 UTC)" (scheduled at 2025-02-23 10:13:06.578731+00:00)
2025-02-23 18:13:06 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 18:13:06 - fastapi - INFO - 开始采集数据
2025-02-23 18:13:25 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:14:43 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:16:00 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:17:00 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-23 18:17:00 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 10:13:06 UTC], next run at: 2025-02-23 10:13:06 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 14, in execute_task
    get_interest_by_region(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 86, in get_interest_by_region
    region_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-23 20:33:33 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 21:47:25 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 21:21:23 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 21:44:20 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:04:16 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:04:52 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:05:29 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:06:31 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:09:18 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:09:46 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:14:50 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:15:02 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
