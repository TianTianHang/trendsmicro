2025-02-23 14:59:49 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:39:29 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:57:38 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:58:45 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 16:59:20 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 16:59:20 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 08:59:20 UTC], next run at: 2025-02-23 08:59:20 UTC)" (scheduled at 2025-02-23 08:59:20.114674+00:00)
2025-02-23 16:59:20 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 16:59:20 - fastapi - INFO - 开始采集数据
2025-02-23 16:59:20 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 08:59:20 UTC], next run at: 2025-02-23 08:59:20 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 16, in execute_task
    get_interest_over_time(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 125, in get_interest_over_time
    timeframe_start = datetime.strptime(start, "%Y-%m-%d %H:%M:%S")
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 349, in _strptime
    raise ValueError("time data %r does not match format %r" %
ValueError: time data '2025-01-01' does not match format '%Y-%m-%d %H:%M:%S'
2025-02-23 17:01:55 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:02:07 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:02:07 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:02:07 UTC], next run at: 2025-02-23 09:02:07 UTC)" (scheduled at 2025-02-23 09:02:07.959934+00:00)
2025-02-23 17:02:08 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:02:08 - fastapi - INFO - 开始采集数据
2025-02-23 17:02:08 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 09:02:07 UTC], next run at: 2025-02-23 09:02:07 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 16, in execute_task
    get_interest_over_time(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 125, in get_interest_over_time
    timeframe_start = datetime.strptime(start, "%Y-%m-%d %H:%M:%S")
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 349, in _strptime
    raise ValueError("time data %r does not match format %r" %
ValueError: time data '2025-01-01' does not match format '%Y-%m-%d %H:%M:%S'
2025-02-23 17:04:04 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:04:12 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:04:12 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:04:12 UTC], next run at: 2025-02-23 09:04:12 UTC)" (scheduled at 2025-02-23 09:04:12.533295+00:00)
2025-02-23 17:04:12 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 17:04:12 - fastapi - INFO - 开始采集数据
2025-02-23 17:04:23 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:04:23 - fastapi - ERROR - Error: strptime() argument 1 must be str, not Timestamp
2025-02-23 17:04:23 - fastapi - INFO - 采集数据结束
2025-02-23 17:04:23 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:04:12 UTC], next run at: 2025-02-23 09:04:12 UTC)" executed successfully
2025-02-23 17:05:51 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:05:51 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:05:51 UTC], next run at: 2025-02-23 09:05:51 UTC)" (scheduled at 2025-02-23 09:05:51.325064+00:00)
2025-02-23 17:05:51 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:05:51 - fastapi - INFO - 开始采集数据
2025-02-23 17:05:53 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:12:23 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:12:59 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:12:59 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:12:59 UTC], next run at: 2025-02-23 09:12:59 UTC)" (scheduled at 2025-02-23 09:12:59.062125+00:00)
2025-02-23 17:12:59 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:12:59 - fastapi - INFO - 开始采集数据
2025-02-23 17:13:23 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 17:14:25 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:14:25 - fastapi - ERROR - Error: 'isPartial'
2025-02-23 17:14:25 - fastapi - INFO - 采集数据结束
2025-02-23 17:14:25 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:12:59 UTC], next run at: 2025-02-23 09:12:59 UTC)" executed successfully
2025-02-23 17:15:21 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:15:21 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:15:21 UTC], next run at: 2025-02-23 09:15:21 UTC)" (scheduled at 2025-02-23 09:15:21.270976+00:00)
2025-02-23 17:15:21 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:15:21 - fastapi - INFO - 开始采集数据
2025-02-23 17:15:22 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:17:24 - fastapi - ERROR - Error: 'isPartial'
2025-02-23 17:17:24 - fastapi - INFO - 采集数据结束
2025-02-23 17:17:24 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:15:21 UTC], next run at: 2025-02-23 09:15:21 UTC)" executed successfully
2025-02-23 17:17:48 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:17:48 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:17:48 UTC], next run at: 2025-02-23 09:17:48 UTC)" (scheduled at 2025-02-23 09:17:48.749147+00:00)
2025-02-23 17:17:48 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:17:48 - fastapi - INFO - 开始采集数据
2025-02-23 17:17:50 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:17:53 - fastapi - ERROR - Error: 'isPartial'
2025-02-23 17:17:53 - fastapi - INFO - 采集数据结束
2025-02-23 17:17:53 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:17:48 UTC], next run at: 2025-02-23 09:17:48 UTC)" executed successfully
2025-02-23 17:18:16 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:18:32 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:18:32 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:18:32 UTC], next run at: 2025-02-23 09:18:32 UTC)" (scheduled at 2025-02-23 09:18:32.520298+00:00)
2025-02-23 17:18:32 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:18:32 - fastapi - INFO - 开始采集数据
2025-02-23 17:18:43 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:18:43 - fastapi - ERROR - Error: (builtins.TypeError) SQLite DateTime type only accepts Python datetime and date objects as input.
[SQL: INSERT INTO time_interest (keywords, geo_code, time, "values", is_partial) VALUES (?, ?, ?, ?, ?) RETURNING id]
[parameters: [{'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-01 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-02 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-03 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-04 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-05 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-06 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(26), np.int64(14)], 'time': '2025-01-07 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(32), np.int64(14)], 'time': '2025-01-08 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(12), np.int64(6)], 'time': '2025-01-09 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(6), np.int64(2)], 'time': '2025-01-10 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(4), np.int64(1)], 'time': '2025-01-11 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(1)], 'time': '2025-01-12 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(1)], 'time': '2025-01-13 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(1)], 'time': '2025-01-14 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(1)], 'time': '2025-01-15 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(1), np.int64(0)], 'time': '2025-01-16 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(0)], 'time': '2025-01-17 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(0)], 'time': '2025-01-18 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(2), np.int64(0)], 'time': '2025-01-19 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(33), np.int64(16)], 'time': '2025-01-20 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(46), np.int64(36)], 'time': '2025-01-21 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(23), np.int64(19)], 'time': '2025-01-22 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(13), np.int64(11)], 'time': '2025-01-23 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(10), np.int64(8)], 'time': '2025-01-24 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(15), np.int64(15)], 'time': '2025-01-25 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(12), np.int64(10)], 'time': '2025-01-26 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(8), np.int64(7)], 'time': '2025-01-27 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(28), np.int64(24)], 'time': '2025-01-28 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(17), np.int64(15)], 'time': '2025-01-29 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(10), np.int64(8)], 'time': '2025-01-30 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(8), np.int64(7)], 'time': '2025-01-31 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(6), np.int64(6)], 'time': '2025-02-01 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(6), np.int64(6)], 'time': '2025-02-02 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(5), np.int64(5)], 'time': '2025-02-03 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(4), np.int64(4)], 'time': '2025-02-04 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(4), np.int64(3)], 'time': '2025-02-05 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(3)], 'time': '2025-02-06 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(3)], 'time': '2025-02-07 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(3), np.int64(3)], 'time': '2025-02-08 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(5), np.int64(8)], 'time': '2025-02-09 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(18), np.int64(35)], 'time': '2025-02-10 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(75), np.int64(69)], 'time': '2025-02-11 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(100), np.int64(43)], 'time': '2025-02-12 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(82), np.int64(29)], 'time': '2025-02-13 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(46), np.int64(16)], 'time': '2025-02-14 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(30), np.int64(12)], 'time': '2025-02-15 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(23), np.int64(9)], 'time': '2025-02-16 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(16), np.int64(7)], 'time': '2025-02-17 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}, {'geo_code': '', 'values': [np.int64(16), np.int64(7)], 'time': '2025-02-18 00:00:00', 'is_partial': False, 'keywords': ['Gulf of Mexico', 'Gulf of America']}]]
2025-02-23 17:18:43 - fastapi - INFO - 采集数据结束
2025-02-23 17:18:43 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:18:32 UTC], next run at: 2025-02-23 09:18:32 UTC)" executed successfully
2025-02-23 17:21:58 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:22:06 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:22:06 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:22:06 UTC], next run at: 2025-02-23 09:22:06 UTC)" (scheduled at 2025-02-23 09:22:06.355900+00:00)
2025-02-23 17:22:06 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 17:22:06 - fastapi - INFO - 开始采集数据
2025-02-23 17:22:06 - fastapi - ERROR - 数据已存在或冲突: (sqlite3.IntegrityError) UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end
[SQL: INSERT INTO request_history (job_type, keywords, geo_code, timeframe_start, timeframe_end, created_at, status) VALUES (?, ?, ?, ?, ?, ?, ?)]
[parameters: ('time', '["Gulf of Mexico", "Gulf of America"]', '', '2025-01-01', '2025-02-18', '2025-02-23', 'created')]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-02-23 17:22:06 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 09:22:06 UTC], next run at: 2025-02-23 09:22:06 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\default.py", line 942, in do_execute
    cursor.execute(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 16, in execute_task
    get_interest_over_time(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 150, in get_interest_over_time
    db.commit()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 2032, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 1313, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 1288, in _prepare_impl
    self.session.flush()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4353, in flush
    self._flush(objects)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4488, in _flush
    with util.safe_reraise():
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4449, in _flush
    flush_context.execute()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\unitofwork.py", line 466, in execute
    rec.execute(self)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1416, in execute
    return meth(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\sql\elements.py", line 515, in _execute_on_connection
    return connection._execute_clauseelement(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1638, in _execute_clauseelement
    ret = self._execute_context(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1843, in _execute_context
    return self._exec_single_context(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1983, in _exec_single_context
    self._handle_dbapi_exception(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 2352, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\default.py", line 942, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end
[SQL: INSERT INTO request_history (job_type, keywords, geo_code, timeframe_start, timeframe_end, created_at, status) VALUES (?, ?, ?, ?, ?, ?, ?)]
[parameters: ('time', '["Gulf of Mexico", "Gulf of America"]', '', '2025-01-01', '2025-02-18', '2025-02-23', 'created')]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-02-23 17:22:42 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:22:42 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:22:42 UTC], next run at: 2025-02-23 09:22:42 UTC)" (scheduled at 2025-02-23 09:22:42.053996+00:00)
2025-02-23 17:22:42 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:22:42 - fastapi - INFO - 开始采集数据
2025-02-23 17:22:45 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:24:20 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 17:24:24 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 17:24:24 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 09:24:24 UTC], next run at: 2025-02-23 09:24:24 UTC)" (scheduled at 2025-02-23 09:24:24.615557+00:00)
2025-02-23 17:24:24 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-23 17:24:24 - fastapi - INFO - 开始采集数据
2025-02-23 17:24:34 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-18, Geo: 
2025-02-23 17:24:34 - fastapi - INFO - 采集数据结束
2025-02-23 17:24:34 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-23 09:24:24 UTC], next run at: 2025-02-23 09:24:24 UTC)" executed successfully
2025-02-23 18:05:48 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 18:05:48 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 10:05:48 UTC], next run at: 2025-02-23 10:05:48 UTC)" (scheduled at 2025-02-23 10:05:48.284175+00:00)
2025-02-23 18:05:48 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 18:05:48 - fastapi - INFO - 开始采集数据
2025-02-23 18:06:05 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:07:21 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:08:38 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:09:38 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-23 18:09:38 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 10:05:48 UTC], next run at: 2025-02-23 10:05:48 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 14, in execute_task
    get_interest_by_region(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 86, in get_interest_by_region
    region_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-23 18:13:06 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-23 18:13:06 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-23 10:13:06 UTC], next run at: 2025-02-23 10:13:06 UTC)" (scheduled at 2025-02-23 10:13:06.578731+00:00)
2025-02-23 18:13:06 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-23 18:13:06 - fastapi - INFO - 开始采集数据
2025-02-23 18:13:25 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:14:43 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:16:00 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-23 18:17:00 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-23 18:17:00 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-23 10:13:06 UTC], next run at: 2025-02-23 10:13:06 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 49, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 14, in execute_task
    get_interest_by_region(keywords, geo_code, interval, start_date, end_date)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 86, in get_interest_by_region
    region_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-23 20:33:33 - apscheduler.scheduler - INFO - Scheduler started
2025-02-23 21:47:25 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 21:21:23 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 21:44:20 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:04:16 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:04:52 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:05:29 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:06:31 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:09:18 - apscheduler.scheduler - INFO - Scheduler started
2025-02-24 22:09:46 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:14:50 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-24 22:15:02 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-25 11:38:51 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 11:51:03 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 11:58:26 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 11:58:26 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 11:58:26 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 03:58:26 UTC], next run at: 2025-02-25 03:58:26 UTC)" (scheduled at 2025-02-25 03:58:26.193921+00:00)
2025-02-25 11:58:26 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 11:58:26 - fastapi - INFO - 开始采集数据
2025-02-25 11:58:26 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 03:58:26 UTC], next run at: 2025-02-25 03:58:26 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 124, in get_interest_over_time
    timeframe_start = datetime.strptime(start, "%Y-%m-%d")
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: T00:00:00
2025-02-25 12:04:18 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 12:04:33 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 12:04:33 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 12:04:33 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 04:04:33 UTC], next run at: 2025-02-25 04:04:33 UTC)" (scheduled at 2025-02-25 04:04:33.976693+00:00)
2025-02-25 12:04:34 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 12:04:34 - fastapi - INFO - 开始采集数据
2025-02-25 12:04:34 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 04:04:33 UTC], next run at: 2025-02-25 04:04:33 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 124, in get_interest_over_time
    timeframe_start = datetime.strptime(start, "%Y-%m-%d")
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains: T00:00:00
2025-02-25 12:07:11 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 12:07:11 - fastapi - INFO - 任务 2 已加入队列等待执行
2025-02-25 12:07:11 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 04:07:10 UTC], next run at: 2025-02-25 04:07:10 UTC)" (scheduled at 2025-02-25 04:07:10.989839+00:00)
2025-02-25 12:07:11 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-25 12:07:11 - fastapi - INFO - 开始采集数据
2025-02-25 12:08:26 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 12:09:23 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 12:09:23 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 12:09:23 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 04:09:23 UTC], next run at: 2025-02-25 04:09:23 UTC)" (scheduled at 2025-02-25 04:09:23.807003+00:00)
2025-02-25 12:09:23 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 12:09:23 - fastapi - INFO - 开始采集数据
2025-02-25 12:11:12 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 12:11:27 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 12:11:27 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 12:11:27 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 04:11:27 UTC], next run at: 2025-02-25 04:11:27 UTC)" (scheduled at 2025-02-25 04:11:27.852877+00:00)
2025-02-25 12:11:27 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 12:11:28 - fastapi - INFO - 开始采集数据
2025-02-25 12:12:52 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 12:13:12 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 12:13:12 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 12:13:12 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 04:13:12 UTC], next run at: 2025-02-25 04:13:12 UTC)" (scheduled at 2025-02-25 04:13:12.108050+00:00)
2025-02-25 12:13:12 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 12:13:12 - fastapi - INFO - 开始采集数据
2025-02-25 12:13:57 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 12:15:14 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 12:16:31 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 12:17:31 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-25 12:17:31 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 04:13:12 UTC], next run at: 2025-02-25 04:13:12 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-25 12:44:07 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 13:36:34 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 13:37:50 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 13:40:21 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 13:40:21 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 13:40:21 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 05:40:21 UTC], next run at: 2025-02-25 05:40:21 UTC)" (scheduled at 2025-02-25 05:40:21.048638+00:00)
2025-02-25 13:40:21 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 13:40:21 - fastapi - INFO - 开始采集数据
2025-02-25 13:40:45 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 13:42:01 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 13:43:18 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 13:44:18 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-25 13:44:19 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 05:40:21 UTC], next run at: 2025-02-25 05:40:21 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-25 14:18:17 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 14:19:10 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 14:19:10 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 06:19:10 UTC], next run at: 2025-02-25 06:19:10 UTC)" (scheduled at 2025-02-25 06:19:10.187440+00:00)
2025-02-25 14:19:10 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 14:19:10 - fastapi - INFO - 开始采集数据
2025-02-25 14:19:10 - fastapi - ERROR - 数据已存在或冲突: (sqlite3.IntegrityError) UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end
[SQL: INSERT INTO request_history (job_type, keywords, geo_code, timeframe_start, timeframe_end, created_at, status) VALUES (?, ?, ?, ?, ?, ?, ?)]
[parameters: ('time', '["Gulf of Mexico", "Gulf of America"]', '', '2025-01-01', '2025-02-25', '2025-02-25', 'created')]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-02-25 14:19:11 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 06:19:10 UTC], next run at: 2025-02-25 06:19:10 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\default.py", line 942, in do_execute
    cursor.execute(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 148, in get_interest_over_time
    db.commit()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 2032, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 1313, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 1288, in _prepare_impl
    self.session.flush()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4353, in flush
    self._flush(objects)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4488, in _flush
    with util.safe_reraise():
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\session.py", line 4449, in _flush
    flush_context.execute()
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\unitofwork.py", line 466, in execute
    rec.execute(self)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\orm\persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1416, in execute
    return meth(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\sql\elements.py", line 515, in _execute_on_connection
    return connection._execute_clauseelement(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1638, in _execute_clauseelement
    ret = self._execute_context(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1843, in _execute_context
    return self._exec_single_context(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1983, in _exec_single_context
    self._handle_dbapi_exception(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 2352, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\engine\default.py", line 942, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: request_history.keywords, request_history.job_type, request_history.geo_code, request_history.timeframe_start, request_history.timeframe_end
[SQL: INSERT INTO request_history (job_type, keywords, geo_code, timeframe_start, timeframe_end, created_at, status) VALUES (?, ?, ?, ?, ?, ?, ?)]
[parameters: ('time', '["Gulf of Mexico", "Gulf of America"]', '', '2025-01-01', '2025-02-25', '2025-02-25', 'created')]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-02-25 14:20:19 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 14:20:19 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 06:20:19 UTC], next run at: 2025-02-25 06:20:19 UTC)" (scheduled at 2025-02-25 06:20:19.307250+00:00)
2025-02-25 14:20:19 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 14:20:19 - fastapi - INFO - 开始采集数据
2025-02-25 14:22:20 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 14:22:39 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 14:22:39 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 06:22:39 UTC], next run at: 2025-02-25 06:22:39 UTC)" (scheduled at 2025-02-25 06:22:39.584579+00:00)
2025-02-25 14:22:39 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 14:22:39 - fastapi - INFO - 开始采集数据
2025-02-25 14:23:06 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:24:22 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:25:38 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:26:38 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-25 14:26:38 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 06:22:39 UTC], next run at: 2025-02-25 06:22:39 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-25 14:27:49 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 14:28:30 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 14:28:30 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 06:28:30 UTC], next run at: 2025-02-25 06:28:30 UTC)" (scheduled at 2025-02-25 06:28:30.202177+00:00)
2025-02-25 14:28:30 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 14:28:30 - fastapi - INFO - 开始采集数据
2025-02-25 14:28:57 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:30:13 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:31:30 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:32:30 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-25 14:32:30 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 06:28:30 UTC], next run at: 2025-02-25 06:28:30 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-25 14:32:30 - apscheduler.scheduler - INFO - Scheduler has been shut down
2025-02-25 14:32:44 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 14:33:22 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 14:33:22 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 06:33:22 UTC], next run at: 2025-02-25 06:33:22 UTC)" (scheduled at 2025-02-25 06:33:22.145180+00:00)
2025-02-25 14:33:22 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 14:33:22 - fastapi - INFO - 开始采集数据
2025-02-25 14:33:46 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:35:03 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:36:39 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:36:53 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 14:37:32 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 14:37:32 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 06:37:31 UTC], next run at: 2025-02-25 06:37:31 UTC)" (scheduled at 2025-02-25 06:37:31.979049+00:00)
2025-02-25 14:37:32 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 14:37:32 - fastapi - INFO - 开始采集数据
2025-02-25 14:37:56 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:39:14 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 14:39:31 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 14:39:31 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 06:39:31 UTC], next run at: 2025-02-25 06:39:31 UTC)" (scheduled at 2025-02-25 06:39:31.136760+00:00)
2025-02-25 14:39:31 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 14:39:31 - fastapi - INFO - 开始采集数据
2025-02-25 14:39:55 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:41:12 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 14:42:48 - fastapi - ERROR - Error: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 14:42:48 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 06:39:31 UTC], next run at: 2025-02-25 06:39:31 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 19, in _call_trends_api_with_retry
    response = api_call_func(**kwargs)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 372, in interest_over_time
    token, data = self._get_token_data(EMBED_TIMESERIES_URL, locals(), headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 300, in _get_token_data
    req 	= self._get(url, params=params, headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 264, in _get
    last_response.raise_for_status()
AttributeError: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 14:42:48 - apscheduler.scheduler - INFO - Scheduler has been shut down
2025-02-25 16:11:35 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 16:12:54 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 16:14:32 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 16:14:32 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 08:14:32 UTC], next run at: 2025-02-25 08:14:32 UTC)" (scheduled at 2025-02-25 08:14:32.541139+00:00)
2025-02-25 16:14:32 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 16:14:32 - fastapi - INFO - 开始采集数据
2025-02-25 16:14:57 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 16:16:14 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 16:17:30 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-25 16:18:30 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-25 16:18:31 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 08:14:32 UTC], next run at: 2025-02-25 08:14:32 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-25 16:18:49 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 16:19:27 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 16:19:27 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 08:19:27 UTC], next run at: 2025-02-25 08:19:27 UTC)" (scheduled at 2025-02-25 08:19:27.264990+00:00)
2025-02-25 16:19:27 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 16:19:27 - fastapi - INFO - 开始采集数据
2025-02-25 16:20:12 - fastapi - ERROR - Error: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 16:20:12 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 08:19:27 UTC], next run at: 2025-02-25 08:19:27 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 19, in _call_trends_api_with_retry
    response = api_call_func(**kwargs)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 372, in interest_over_time
    token, data = self._get_token_data(EMBED_TIMESERIES_URL, locals(), headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 300, in _get_token_data
    req 	= self._get(url, params=params, headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 264, in _get
    last_response.raise_for_status()
AttributeError: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 16:20:55 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 16:20:55 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 08:20:55 UTC], next run at: 2025-02-25 08:20:55 UTC)" (scheduled at 2025-02-25 08:20:55.666501+00:00)
2025-02-25 16:20:55 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 16:20:55 - fastapi - INFO - 开始采集数据
2025-02-25 16:25:18 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 16:25:27 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 16:25:27 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 08:25:27 UTC], next run at: 2025-02-25 08:25:27 UTC)" (scheduled at 2025-02-25 08:25:27.770672+00:00)
2025-02-25 16:25:27 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 16:25:27 - fastapi - INFO - 开始采集数据
2025-02-25 16:25:54 - fastapi - ERROR - Error: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 16:25:54 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 08:25:27 UTC], next run at: 2025-02-25 08:25:27 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 19, in _call_trends_api_with_retry
    response = api_call_func(**kwargs)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 372, in interest_over_time
    token, data = self._get_token_data(EMBED_TIMESERIES_URL, locals(), headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 300, in _get_token_data
    req 	= self._get(url, params=params, headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 264, in _get
    last_response.raise_for_status()
AttributeError: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 16:27:07 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 16:27:07 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 08:27:07 UTC], next run at: 2025-02-25 08:27:07 UTC)" (scheduled at 2025-02-25 08:27:07.837897+00:00)
2025-02-25 16:27:07 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 16:27:07 - fastapi - INFO - 开始采集数据
2025-02-25 16:28:11 - fastapi - ERROR - Error: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 16:28:11 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 08:27:07 UTC], next run at: 2025-02-25 08:27:07 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 19, in _call_trends_api_with_retry
    response = api_call_func(**kwargs)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 372, in interest_over_time
    token, data = self._get_token_data(EMBED_TIMESERIES_URL, locals(), headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 300, in _get_token_data
    req 	= self._get(url, params=params, headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 264, in _get
    last_response.raise_for_status()
AttributeError: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 16:28:41 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 16:28:41 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 08:28:41 UTC], next run at: 2025-02-25 08:28:41 UTC)" (scheduled at 2025-02-25 08:28:41.614230+00:00)
2025-02-25 16:28:41 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 16:28:41 - fastapi - INFO - 开始采集数据
2025-02-25 16:29:45 - fastapi - ERROR - Error: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 16:29:45 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-25 08:28:41 UTC], next run at: 2025-02-25 08:28:41 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 131, in run_job
    retval = job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 56, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 32, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 157, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 19, in _call_trends_api_with_retry
    response = api_call_func(**kwargs)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 372, in interest_over_time
    token, data = self._get_token_data(EMBED_TIMESERIES_URL, locals(), headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 300, in _get_token_data
    req 	= self._get(url, params=params, headers=headers)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\trendspy\client.py", line 264, in _get
    last_response.raise_for_status()
AttributeError: 'NoneType' object has no attribute 'raise_for_status'
2025-02-25 18:55:56 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:00:03 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:00:03 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:00:03 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:00:03 UTC], next run at: 2025-02-25 11:00:03 UTC)" (scheduled at 2025-02-25 11:00:03.136073+00:00)
2025-02-25 19:00:03 - fastapi - INFO - 开始采集数据
2025-02-25 19:00:17 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-25, Geo: 
2025-02-25 19:00:17 - fastapi - INFO - 采集数据结束
2025-02-25 19:00:17 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:00:03 UTC], next run at: 2025-02-25 11:00:03 UTC)" executed successfully
2025-02-25 19:03:05 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:03:05 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:03:05 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:03:05 UTC], next run at: 2025-02-25 11:03:05 UTC)" (scheduled at 2025-02-25 11:03:05.781713+00:00)
2025-02-25 19:03:05 - fastapi - INFO - 开始采集数据
2025-02-25 19:03:17 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-25, Geo: 
2025-02-25 19:08:11 - fastapi - INFO - 采集数据结束
2025-02-25 19:08:17 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:03:05 UTC], next run at: 2025-02-25 11:03:05 UTC)" executed successfully
2025-02-25 19:10:07 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:10:43 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:11:22 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:11:30 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:13:16 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:13:33 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:14:29 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:14:34 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:22:30 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:24:02 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:25:00 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:25:07 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:25:07 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:25:07 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:25:07 UTC], next run at: 2025-02-25 11:25:07 UTC)" (scheduled at 2025-02-25 11:25:07.791585+00:00)
2025-02-25 19:25:11 - fastapi - INFO - 开始采集数据
2025-02-25 19:25:11 - fastapi - INFO - 采集数据结束
2025-02-25 19:25:11 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:25:07 UTC], next run at: 2025-02-25 11:25:07 UTC)" executed successfully
2025-02-25 19:28:23 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:28:50 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:32:37 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:32:44 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:32:44 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:32:44 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:32:44 UTC], next run at: 2025-02-25 11:32:44 UTC)" (scheduled at 2025-02-25 11:32:44.603555+00:00)
2025-02-25 19:32:48 - fastapi - INFO - 开始采集数据
2025-02-25 19:32:48 - fastapi - INFO - 采集数据结束
2025-02-25 19:32:48 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:32:44 UTC], next run at: 2025-02-25 11:32:44 UTC)" executed successfully
2025-02-25 19:33:06 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:33:06 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:33:06 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:33:06 UTC], next run at: 2025-02-25 11:33:06 UTC)" (scheduled at 2025-02-25 11:33:06.922556+00:00)
2025-02-25 19:33:06 - fastapi - INFO - 开始采集数据
2025-02-25 19:33:07 - fastapi - INFO - 采集数据结束
2025-02-25 19:33:07 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:33:06 UTC], next run at: 2025-02-25 11:33:06 UTC)" executed successfully
2025-02-25 19:33:27 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:33:27 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:33:27 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:33:27 UTC], next run at: 2025-02-25 11:33:27 UTC)" (scheduled at 2025-02-25 11:33:27.561704+00:00)
2025-02-25 19:33:27 - fastapi - INFO - 开始采集数据
2025-02-25 19:33:27 - fastapi - INFO - 采集数据结束
2025-02-25 19:33:27 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:33:27 UTC], next run at: 2025-02-25 11:33:27 UTC)" executed successfully
2025-02-25 19:33:44 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:33:44 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:33:44 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:33:44 UTC], next run at: 2025-02-25 11:33:44 UTC)" (scheduled at 2025-02-25 11:33:44.746498+00:00)
2025-02-25 19:33:44 - fastapi - INFO - 开始采集数据
2025-02-25 19:33:44 - fastapi - INFO - 采集数据结束
2025-02-25 19:33:44 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:33:44 UTC], next run at: 2025-02-25 11:33:44 UTC)" executed successfully
2025-02-25 19:38:19 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:40:12 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:40:12 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:40:12 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:40:12 UTC], next run at: 2025-02-25 11:40:12 UTC)" (scheduled at 2025-02-25 11:40:12.922371+00:00)
2025-02-25 19:40:13 - fastapi - INFO - 开始采集数据
2025-02-25 19:40:13 - fastapi - INFO - 采集数据结束
2025-02-25 19:40:13 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:40:12 UTC], next run at: 2025-02-25 11:40:12 UTC)" executed successfully
2025-02-25 19:40:53 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:42:36 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:42:47 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:42:47 - fastapi - ERROR - 任务 1 加入队列失败: object NoneType can't be used in 'await' expression
2025-02-25 19:42:47 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:42:47 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:42:47 UTC], next run at: 2025-02-25 11:42:47 UTC)" (scheduled at 2025-02-25 11:42:47.326301+00:00)
2025-02-25 19:42:47 - fastapi - INFO - 开始采集数据
2025-02-25 19:43:41 - fastapi - INFO - 采集数据结束
2025-02-25 19:43:59 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:42:47 UTC], next run at: 2025-02-25 11:42:47 UTC)" executed successfully
2025-02-25 19:43:59 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-10' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=TypeError('Boolean value of this clause is not defined')>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 33, in handle_finish_tasks
    result=db.query(TimeInterest).filter(TimeInterest.id in task.get("interest_id"))
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\sql\elements.py", line 3946, in __bool__
    raise TypeError("Boolean value of this clause is not defined")
TypeError: Boolean value of this clause is not defined
2025-02-25 19:44:47 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:44:47 - fastapi - ERROR - 任务 1 加入队列失败: object NoneType can't be used in 'await' expression
2025-02-25 19:44:47 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:44:47 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:44:47 UTC], next run at: 2025-02-25 11:44:47 UTC)" (scheduled at 2025-02-25 11:44:47.281578+00:00)
2025-02-25 19:44:47 - fastapi - INFO - 开始采集数据
2025-02-25 19:45:00 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-25, Geo: 
2025-02-25 19:45:00 - fastapi - INFO - 采集数据结束
2025-02-25 19:45:00 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:44:47 UTC], next run at: 2025-02-25 11:44:47 UTC)" executed successfully
2025-02-25 19:45:52 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-19' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...erests': [], 'meta': []}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 36, in handle_finish_tasks
    req=NotifyRequest(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...erests': [], 'meta': []}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-25 19:47:19 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:47:19 - fastapi - ERROR - 任务 1 加入队列失败: object NoneType can't be used in 'await' expression
2025-02-25 19:47:19 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:47:19 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:47:19 UTC], next run at: 2025-02-25 11:47:19 UTC)" (scheduled at 2025-02-25 11:47:19.828235+00:00)
2025-02-25 19:47:19 - fastapi - INFO - 开始采集数据
2025-02-25 19:47:25 - fastapi - INFO - 采集数据结束
2025-02-25 19:47:25 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:47:19 UTC], next run at: 2025-02-25 11:47:19 UTC)" executed successfully
2025-02-25 19:47:25 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-26' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=TypeError('Boolean value of this clause is not defined')>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 33, in handle_finish_tasks
    result=db.query(TimeInterest).filter(TimeInterest.id in task.get("interest_id"))
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\sqlalchemy\sql\elements.py", line 3946, in __bool__
    raise TypeError("Boolean value of this clause is not defined")
TypeError: Boolean value of this clause is not defined
2025-02-25 19:47:50 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:47:50 - fastapi - ERROR - 任务 1 加入队列失败: object NoneType can't be used in 'await' expression
2025-02-25 19:47:50 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:47:50 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:47:50 UTC], next run at: 2025-02-25 11:47:50 UTC)" (scheduled at 2025-02-25 11:47:50.099848+00:00)
2025-02-25 19:47:50 - fastapi - INFO - 开始采集数据
2025-02-25 19:49:41 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:50:10 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:50:10 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 19:50:10 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:50:10 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:50:10 UTC], next run at: 2025-02-25 11:50:10 UTC)" (scheduled at 2025-02-25 11:50:10.477981+00:00)
2025-02-25 19:50:10 - fastapi - INFO - 开始采集数据
2025-02-25 19:50:29 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['Gulf of Mexico', 'Gulf of America'], Timeframe: 2025-01-01 2025-02-25, Geo: 
2025-02-25 19:50:29 - fastapi - INFO - 采集数据结束
2025-02-25 19:50:29 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:50:10 UTC], next run at: 2025-02-25 11:50:10 UTC)" executed successfully
2025-02-25 19:51:59 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-10' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...erests': [], 'meta': []}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 36, in handle_finish_tasks
    req=NotifyRequest(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...erests': [], 'meta': []}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-25 19:53:01 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:53:01 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 19:53:01 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:53:01 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:53:01 UTC], next run at: 2025-02-25 11:53:01 UTC)" (scheduled at 2025-02-25 11:53:01.647625+00:00)
2025-02-25 19:53:01 - fastapi - INFO - 开始采集数据
2025-02-25 19:53:01 - fastapi - INFO - 采集数据结束
2025-02-25 19:53:01 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:53:01 UTC], next run at: 2025-02-25 11:53:01 UTC)" executed successfully
2025-02-25 19:55:36 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-18' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 36, in handle_finish_tasks
    req=NotifyRequest(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-25 19:55:54 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:55:54 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 19:55:54 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:55:54 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:55:54 UTC], next run at: 2025-02-25 11:55:54 UTC)" (scheduled at 2025-02-25 11:55:54.514206+00:00)
2025-02-25 19:55:54 - fastapi - INFO - 开始采集数据
2025-02-25 19:55:54 - fastapi - INFO - 采集数据结束
2025-02-25 19:55:54 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:55:54 UTC], next run at: 2025-02-25 11:55:54 UTC)" executed successfully
2025-02-25 19:58:34 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-26' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...erests': [], 'meta': []}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 36, in handle_finish_tasks
    req=NotifyRequest(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...erests': [], 'meta': []}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-25 19:58:55 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:59:28 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 19:59:39 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 19:59:39 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 19:59:39 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 19:59:39 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 11:59:39 UTC], next run at: 2025-02-25 11:59:39 UTC)" (scheduled at 2025-02-25 11:59:39.489615+00:00)
2025-02-25 19:59:39 - fastapi - INFO - 开始采集数据
2025-02-25 19:59:39 - fastapi - INFO - 采集数据结束
2025-02-25 19:59:39 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 11:59:39 UTC], next run at: 2025-02-25 11:59:39 UTC)" executed successfully
2025-02-25 20:00:00 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-10' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 42, in handle_finish_tasks
    req = NotifyRequest(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-25 20:00:23 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 20:00:23 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 20:00:23 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 20:00:23 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 12:00:23 UTC], next run at: 2025-02-25 12:00:23 UTC)" (scheduled at 2025-02-25 12:00:23.126217+00:00)
2025-02-25 20:00:23 - fastapi - INFO - 开始采集数据
2025-02-25 20:00:23 - fastapi - INFO - 采集数据结束
2025-02-25 20:00:23 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 12:00:23 UTC], next run at: 2025-02-25 12:00:23 UTC)" executed successfully
2025-02-25 20:01:10 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-18' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 42, in handle_finish_tasks
    req = NotifyRequest(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-25 20:01:27 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 20:01:27 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 20:01:27 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 20:01:27 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 12:01:27 UTC], next run at: 2025-02-25 12:01:27 UTC)" (scheduled at 2025-02-25 12:01:27.189653+00:00)
2025-02-25 20:01:27 - fastapi - INFO - 开始采集数据
2025-02-25 20:01:27 - fastapi - INFO - 采集数据结束
2025-02-25 20:01:27 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 12:01:27 UTC], next run at: 2025-02-25 12:01:27 UTC)" executed successfully
2025-02-25 20:01:27 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-26' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 212, in handle
    await loop.run_in_executor(None, functools.partial(handler, event))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 42, in handle_finish_tasks
    req = NotifyRequest(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for NotifyRequest
interest_type
  Field required [type=missing, input_value={'task_id': 1, 'type': 'h...ime.date(2025, 2, 25))]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-25 20:02:41 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 20:02:57 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 20:02:57 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 20:02:57 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 20:02:57 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 12:02:57 UTC], next run at: 2025-02-25 12:02:57 UTC)" (scheduled at 2025-02-25 12:02:57.615497+00:00)
2025-02-25 20:02:57 - fastapi - INFO - 开始采集数据
2025-02-25 20:02:57 - fastapi - INFO - 采集数据结束
2025-02-25 20:02:57 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 12:02:57 UTC], next run at: 2025-02-25 12:02:57 UTC)" executed successfully
2025-02-25 20:03:49 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 20:03:53 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 20:03:53 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 20:03:53 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 20:03:53 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 12:03:53 UTC], next run at: 2025-02-25 12:03:53 UTC)" (scheduled at 2025-02-25 12:03:53.644380+00:00)
2025-02-25 20:03:53 - fastapi - INFO - 开始采集数据
2025-02-25 20:03:53 - fastapi - INFO - 采集数据结束
2025-02-25 20:03:53 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 12:03:53 UTC], next run at: 2025-02-25 12:03:53 UTC)" executed successfully
2025-02-25 20:04:36 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 20:04:36 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 20:04:36 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 20:04:36 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 12:04:36 UTC], next run at: 2025-02-25 12:04:36 UTC)" (scheduled at 2025-02-25 12:04:36.770763+00:00)
2025-02-25 20:04:36 - fastapi - INFO - 开始采集数据
2025-02-25 20:04:36 - fastapi - INFO - 采集数据结束
2025-02-25 20:04:36 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 12:04:36 UTC], next run at: 2025-02-25 12:04:36 UTC)" executed successfully
2025-02-25 20:04:58 - apscheduler.scheduler - INFO - Scheduler started
2025-02-25 20:05:10 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 20:05:10 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 20:05:10 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 20:05:10 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 12:05:10 UTC], next run at: 2025-02-25 12:05:10 UTC)" (scheduled at 2025-02-25 12:05:10.463573+00:00)
2025-02-25 20:05:10 - fastapi - INFO - 开始采集数据
2025-02-25 20:05:10 - fastapi - INFO - 采集数据结束
2025-02-25 20:05:10 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 12:05:10 UTC], next run at: 2025-02-25 12:05:10 UTC)" executed successfully
2025-02-25 20:05:39 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-25 20:05:39 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-25 20:05:39 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-25 20:05:39 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-25 12:05:39 UTC], next run at: 2025-02-25 12:05:39 UTC)" (scheduled at 2025-02-25 12:05:39.360352+00:00)
2025-02-25 20:05:39 - fastapi - INFO - 开始采集数据
2025-02-25 20:05:39 - fastapi - INFO - 采集数据结束
2025-02-25 20:05:39 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-25 12:05:39 UTC], next run at: 2025-02-25 12:05:39 UTC)" executed successfully
2025-02-26 14:44:14 - apscheduler.scheduler - INFO - Scheduler started
2025-02-26 14:52:50 - apscheduler.scheduler - INFO - Scheduler started
2025-02-26 14:57:55 - apscheduler.scheduler - INFO - Scheduler started
2025-02-26 15:13:56 - apscheduler.scheduler - INFO - Scheduler started
2025-02-26 15:40:50 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-26 15:40:50 - fastapi - INFO - 任务 2 已加入队列等待执行
2025-02-26 15:40:50 - apscheduler.scheduler - INFO - Removed job historical_2
2025-02-26 15:40:50 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-26 07:40:50 UTC], next run at: 2025-02-26 07:40:50 UTC)" (scheduled at 2025-02-26 07:40:50.691991+00:00)
2025-02-26 15:40:50 - fastapi - INFO - 开始采集数据
2025-02-26 15:41:15 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-26 15:42:31 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-26 15:43:47 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-02-26 15:44:47 - fastapi - ERROR - Error: Max retries exceeded for API call
2025-02-26 15:44:47 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-26 07:40:50 UTC], next run at: 2025-02-26 07:40:50 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 181, in run_coroutine_job
    retval = await job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 57, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 160, in get_interest_over_time
    time_data = _call_trends_api_with_retry(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 36, in _call_trends_api_with_retry
    raise Exception("Max retries exceeded for API call")
Exception: Max retries exceeded for API call
2025-02-26 15:44:47 - asyncio - ERROR - Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)
handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>
Traceback (most recent call last):
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\asyncio\events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\asyncio\proactor_events.py", line 165, in _call_connection_lost
    self._sock.shutdown(socket.SHUT_RDWR)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-02-27 10:09:23 - apscheduler.scheduler - INFO - Scheduler started
2025-02-27 10:10:09 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 15:06:44 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 19:06:57 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 19:42:43 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 19:46:25 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 19:48:38 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 19:55:07 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 19:57:47 - apscheduler.scheduler - INFO - Removed job scheduled_1
2025-02-28 19:59:15 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 20:02:04 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 20:02:29 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 20:07:29 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 20:17:59 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 20:18:27 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 20:18:27 - fastapi - INFO - interval_config: {'minutes': 1}
2025-02-28 20:19:27 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:20:27 CST)" (scheduled at 2025-02-28 20:19:27.471814+08:00)
2025-02-28 20:19:44 - apscheduler.executors.default - ERROR - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:20:27 CST)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 181, in run_coroutine_job
    retval = await job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 81, in execute_scheduled_task
    raise e
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 70, in execute_scheduled_task
    end_date=datetime().strftime("%Y-%m-%d"),
TypeError: function missing required argument 'year' (pos 1)
2025-02-28 20:20:18 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 20:20:31 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 20:20:31 - fastapi - INFO - interval_config: {'minutes': 1}
2025-02-28 20:21:31 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:22:31 CST)" (scheduled at 2025-02-28 20:21:31.716268+08:00)
2025-02-28 20:22:07 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 20:22:31 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:23:31 CST)" (scheduled at 2025-02-28 20:22:31.716268+08:00)
2025-02-28 20:22:42 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:23:31 CST)" executed successfully
2025-02-28 20:22:42 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-28 20:22:42 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-28 20:22:42 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-28 20:22:42 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-28 12:22:42 UTC], next run at: 2025-02-28 12:22:42 UTC)" (scheduled at 2025-02-28 12:22:42.745013+00:00)
2025-02-28 20:22:42 - fastapi - INFO - 开始采集数据
2025-02-28 20:22:42 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-28 12:22:42 UTC], next run at: 2025-02-28 12:22:42 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 181, in run_coroutine_job
    retval = await job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 57, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 17, in execute_task
    raise ValueError(f"无效的任务类型: {job_type}")
ValueError: 无效的任务类型: string
2025-02-28 20:23:31 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:24:31 CST)" (scheduled at 2025-02-28 20:23:31.716268+08:00)
2025-02-28 20:23:40 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:24:31 CST)" executed successfully
2025-02-28 20:23:40 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-28 20:23:40 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-02-28 20:23:40 - apscheduler.scheduler - INFO - Removed job historical_1
2025-02-28 20:23:40 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-28 12:23:40 UTC], next run at: 2025-02-28 12:23:40 UTC)" (scheduled at 2025-02-28 12:23:40.607862+00:00)
2025-02-28 20:23:40 - fastapi - INFO - 开始采集数据
2025-02-28 20:23:40 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-02-28 12:23:40 UTC], next run at: 2025-02-28 12:23:40 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 181, in run_coroutine_job
    retval = await job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 57, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 17, in execute_task
    raise ValueError(f"无效的任务类型: {job_type}")
ValueError: 无效的任务类型: string
2025-02-28 20:24:38 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 20:24:41 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 20:24:41 - fastapi - INFO - interval_config: {'minutes': 1}
2025-02-28 20:25:41 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:26:41 CST)" (scheduled at 2025-02-28 20:25:41.738788+08:00)
2025-02-28 20:25:41 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:26:41 CST)" executed successfully
2025-02-28 20:26:45 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 20:26:49 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-02-28 20:26:49 - fastapi - INFO - interval_config: {'minutes': 1}
2025-02-28 20:27:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:28:49 CST)" (scheduled at 2025-02-28 20:27:49.396668+08:00)
2025-02-28 20:27:49 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:28:49 CST)" executed successfully
2025-02-28 20:28:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:29:49 CST)" (scheduled at 2025-02-28 20:28:49.396668+08:00)
2025-02-28 20:28:49 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:29:49 CST)" executed successfully
2025-02-28 20:29:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:30:49 CST)" (scheduled at 2025-02-28 20:29:49.396668+08:00)
2025-02-28 20:29:54 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:30:49 CST)" executed successfully
2025-02-28 20:30:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:31:49 CST)" (scheduled at 2025-02-28 20:30:49.396668+08:00)
2025-02-28 20:30:49 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:31:49 CST)" executed successfully
2025-02-28 20:31:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:32:49 CST)" (scheduled at 2025-02-28 20:31:49.396668+08:00)
2025-02-28 20:31:49 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:32:49 CST)" executed successfully
2025-02-28 20:32:32 - apscheduler.scheduler - INFO - Scheduler started
2025-02-28 20:32:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:33:49 CST)" (scheduled at 2025-02-28 20:32:49.396668+08:00)
2025-02-28 20:32:49 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:33:49 CST)" executed successfully
2025-02-28 20:35:59 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-28 20:35:59 - fastapi - INFO - 任务 6 已加入队列等待执行
2025-02-28 20:35:59 - apscheduler.scheduler - INFO - Removed job historical_6
2025-02-28 20:35:59 - apscheduler.executors.default - WARNING - Run time of job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:36:49 CST)" was missed by 0:02:10.085026
2025-02-28 20:35:59 - apscheduler.executors.default - WARNING - Run time of job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:36:49 CST)" was missed by 0:01:10.086529
2025-02-28 20:35:59 - apscheduler.executors.default - WARNING - Run time of job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:36:49 CST)" was missed by 0:00:10.087188
2025-02-28 20:35:59 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-28 12:35:59 UTC], next run at: 2025-02-28 12:35:59 UTC)" (scheduled at 2025-02-28 12:35:59.435542+00:00)
2025-02-28 20:35:59 - fastapi - INFO - 开始采集数据
2025-02-28 20:36:09 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['deepseek', 'deepsek'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-02-28 20:36:09 - fastapi - INFO - 采集数据结束
2025-02-28 20:36:09 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-28 12:35:59 UTC], next run at: 2025-02-28 12:35:59 UTC)" executed successfully
2025-02-28 20:36:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:37:49 CST)" (scheduled at 2025-02-28 20:36:49.396668+08:00)
2025-02-28 20:36:49 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:37:49 CST)" executed successfully
2025-02-28 20:36:49 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-28 20:36:49 - fastapi - INFO - 任务 7 已加入队列等待执行
2025-02-28 20:36:49 - apscheduler.scheduler - INFO - Removed job historical_7
2025-02-28 20:36:49 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-28 12:36:49 UTC], next run at: 2025-02-28 12:36:49 UTC)" (scheduled at 2025-02-28 12:36:49.432057+00:00)
2025-02-28 20:36:49 - fastapi - INFO - 开始采集数据
2025-02-28 20:36:49 - fastapi - INFO - 采集数据结束
2025-02-28 20:36:49 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-28 12:36:49 UTC], next run at: 2025-02-28 12:36:49 UTC)" executed successfully
2025-02-28 20:37:49 - apscheduler.executors.default - INFO - Running job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:38:49 CST)" (scheduled at 2025-02-28 20:37:49.396668+08:00)
2025-02-28 20:37:49 - apscheduler.executors.default - INFO - Job "execute_scheduled_task (trigger: interval[0:01:00], next run at: 2025-02-28 20:38:49 CST)" executed successfully
2025-02-28 20:37:49 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-02-28 20:37:49 - fastapi - INFO - 任务 8 已加入队列等待执行
2025-02-28 20:37:49 - apscheduler.scheduler - INFO - Removed job historical_8
2025-02-28 20:37:49 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-02-28 12:37:49 UTC], next run at: 2025-02-28 12:37:49 UTC)" (scheduled at 2025-02-28 12:37:49.424909+00:00)
2025-02-28 20:37:49 - fastapi - INFO - 开始采集数据
2025-02-28 20:37:49 - fastapi - INFO - 采集数据结束
2025-02-28 20:37:49 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-02-28 12:37:49 UTC], next run at: 2025-02-28 12:37:49 UTC)" executed successfully
2025-03-02 10:29:34 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 19:35:03 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 20:20:24 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 20:22:14 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:22:14 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:22:14 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:22:14 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:22:14 UTC], next run at: 2025-03-02 12:22:14 UTC)" (scheduled at 2025-03-02 12:22:14.186707+00:00)
2025-03-02 20:22:14 - fastapi - INFO - 开始采集数据
2025-03-02 20:22:45 - fastapi - INFO - Data fetch successful for API function: interest_over_time, with parameters - Keywords: ['new', 'novel'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-02 20:22:45 - fastapi - INFO - 采集数据结束
2025-03-02 20:22:45 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:22:14 UTC], next run at: 2025-03-02 12:22:14 UTC)" executed successfully
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2300, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 51735, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABDCC0>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABDCC0> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2300, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 51735, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2300, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 51735, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2300, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 51735, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:22:45 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=False, error-arg=None; pending-error=ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:22:45 - pika.connection - ERROR - Connection closed while authenticating indicating a probable authentication error
2025-03-02 20:22:45 - pika.connection - INFO - Connection setup terminated due to ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnector - reporting failure: AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('127.0.0.1', 5672)
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2300, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 51735, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2056, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51736), raddr=('127.0.0.1', 5672)>
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABDAB0>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABDAB0> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2056, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51736), raddr=('127.0.0.1', 5672)>
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2056, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51736), raddr=('127.0.0.1', 5672)>
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2056, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51736), raddr=('127.0.0.1', 5672)>
2025-03-02 20:22:45 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=False, error-arg=None; pending-error=ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:22:45 - pika.connection - ERROR - Connection closed while authenticating indicating a probable authentication error
2025-03-02 20:22:45 - pika.connection - INFO - Connection setup terminated due to ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnector - reporting failure: AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - ERROR - AMQP connection workflow failed: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",).
2025-03-02 20:22:45 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnectionWorkflow - reporting failure: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:22:45 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2056, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51736), raddr=('127.0.0.1', 5672)>
2025-03-02 20:22:45 - pika.adapters.blocking_connection - ERROR - Connection workflow failed: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:22:45 - pika.adapters.blocking_connection - ERROR - Error in _create_connection().
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 451, in _create_connection
    raise self._reap_last_connection_workflow_error(error)
pika.exceptions.ProbableAuthenticationError: ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:22:45 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-54' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 208, in handle
    await handler(event, **values)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 55, in handle_finish_tasks
    with RabbitMQClient("interest_data") as client:
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\services\rabbitmq.py", line 55, in __enter__
    self.connect()
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\services\rabbitmq.py", line 26, in connect
    self.connection = pika.BlockingConnection(parameters)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 360, in __init__
    self._impl = self._create_connection(parameters, _impl_class)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 451, in _create_connection
    raise self._reap_last_connection_workflow_error(error)
pika.exceptions.ProbableAuthenticationError: ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:24:33 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:24:33 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:24:33 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:24:33 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:24:33 UTC], next run at: 2025-03-02 12:24:33 UTC)" (scheduled at 2025-03-02 12:24:33.067837+00:00)
2025-03-02 20:24:33 - fastapi - INFO - 开始采集数据
2025-03-02 20:24:33 - fastapi - INFO - 采集数据结束
2025-03-02 20:24:33 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:24:33 UTC], next run at: 2025-03-02 12:24:33 UTC)" executed successfully
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2420, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64087, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EB67F40>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EB67F40> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2420, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64087, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2420, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64087, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2420, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64087, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:24:33 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=False, error-arg=None; pending-error=ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:24:33 - pika.connection - ERROR - Connection closed while authenticating indicating a probable authentication error
2025-03-02 20:24:33 - pika.connection - INFO - Connection setup terminated due to ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnector - reporting failure: AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('127.0.0.1', 5672)
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2420, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64087, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2412, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64088), raddr=('127.0.0.1', 5672)>
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EB67D90>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EB67D90> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2412, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64088), raddr=('127.0.0.1', 5672)>
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2412, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64088), raddr=('127.0.0.1', 5672)>
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2412, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64088), raddr=('127.0.0.1', 5672)>
2025-03-02 20:24:33 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=False, error-arg=None; pending-error=ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:24:33 - pika.connection - ERROR - Connection closed while authenticating indicating a probable authentication error
2025-03-02 20:24:33 - pika.connection - INFO - Connection setup terminated due to ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnector - reporting failure: AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - ERROR - AMQP connection workflow failed: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",).
2025-03-02 20:24:33 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnectionWorkflow - reporting failure: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:24:33 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2412, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64088), raddr=('127.0.0.1', 5672)>
2025-03-02 20:24:33 - pika.adapters.blocking_connection - ERROR - Connection workflow failed: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:24:33 - pika.adapters.blocking_connection - ERROR - Error in _create_connection().
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 451, in _create_connection
    raise self._reap_last_connection_workflow_error(error)
pika.exceptions.ProbableAuthenticationError: ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:24:33 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-98' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 208, in handle
    await handler(event, **values)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 55, in handle_finish_tasks
    with RabbitMQClient("interest_data") as client:
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\services\rabbitmq.py", line 55, in __enter__
    self.connect()
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\services\rabbitmq.py", line 26, in connect
    self.connection = pika.BlockingConnection(parameters)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 360, in __init__
    self._impl = self._create_connection(parameters, _impl_class)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 451, in _create_connection
    raise self._reap_last_connection_workflow_error(error)
pika.exceptions.ProbableAuthenticationError: ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:25:03 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:25:03 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:25:04 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:25:04 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:25:03 UTC], next run at: 2025-03-02 12:25:03 UTC)" (scheduled at 2025-03-02 12:25:03.973283+00:00)
2025-03-02 20:25:04 - fastapi - INFO - 开始采集数据
2025-03-02 20:25:04 - fastapi - INFO - 采集数据结束
2025-03-02 20:25:04 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:25:03 UTC], next run at: 2025-03-02 12:25:03 UTC)" executed successfully
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2704, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64222, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABED70>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABED70> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2704, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64222, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2704, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64222, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2704, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64222, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:25:33 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=False, error-arg=None; pending-error=ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:25:33 - pika.connection - ERROR - Connection closed while authenticating indicating a probable authentication error
2025-03-02 20:25:33 - pika.connection - INFO - Connection setup terminated due to ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnector - reporting failure: AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('127.0.0.1', 5672)
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2704, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 64222, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2532, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64223), raddr=('127.0.0.1', 5672)>
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABEDA0>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000028B0EABEDA0> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2532, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64223), raddr=('127.0.0.1', 5672)>
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2532, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64223), raddr=('127.0.0.1', 5672)>
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2532, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64223), raddr=('127.0.0.1', 5672)>
2025-03-02 20:25:33 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=False, error-arg=None; pending-error=ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:25:33 - pika.connection - ERROR - Connection closed while authenticating indicating a probable authentication error
2025-03-02 20:25:33 - pika.connection - INFO - Connection setup terminated due to ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnector - reporting failure: AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - ERROR - AMQP connection workflow failed: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",).
2025-03-02 20:25:33 - pika.adapters.utils.connection_workflow - ERROR - AMQPConnectionWorkflow - reporting failure: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:25:33 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2532, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 64223), raddr=('127.0.0.1', 5672)>
2025-03-02 20:25:33 - pika.adapters.blocking_connection - ERROR - Connection workflow failed: AMQPConnectionWorkflowFailed: 2 exceptions in all; last exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",); first exception - AMQPConnectorAMQPHandshakeError: ProbableAuthenticationError: Client was disconnected at a connection stage indicating a probable authentication error: ("ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'",)
2025-03-02 20:25:33 - pika.adapters.blocking_connection - ERROR - Error in _create_connection().
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 451, in _create_connection
    raise self._reap_last_connection_workflow_error(error)
pika.exceptions.ProbableAuthenticationError: ConnectionClosedByBroker: (403) 'ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.'
2025-03-02 20:26:57 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 20:28:21 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:28:21 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:28:21 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:28:21 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:28:21 UTC], next run at: 2025-03-02 12:28:21 UTC)" (scheduled at 2025-03-02 12:28:21.864927+00:00)
2025-03-02 20:28:22 - fastapi - INFO - 开始采集数据
2025-03-02 20:28:22 - fastapi - INFO - 采集数据结束
2025-03-02 20:28:22 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:28:21 UTC], next run at: 2025-03-02 12:28:21 UTC)" executed successfully
2025-03-02 20:28:30 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:28:30 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2144, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55573, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:28:30 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001CAD8D1E290>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001CAD8D1E290> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:28:30 - pika.adapters.utils.connection_workflow - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001CAD8D1E290> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:28:30 - pika.adapters.utils.connection_workflow - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001CAD8D1E290> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:28:30 - pika.adapters.blocking_connection - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001CAD8D1E290> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:28:34 - pika.adapters.blocking_connection - INFO - Created channel=1
2025-03-02 20:28:38 - pika.adapters.blocking_connection - INFO - Closing connection (200): Normal shutdown
2025-03-02 20:28:38 - pika.channel - INFO - Closing channel (200): 'Normal shutdown' on <Channel number=1 OPEN conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001CAD8D1E290> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:28:38 - pika.channel - INFO - Received <Channel.CloseOk> on <Channel number=1 CLOSING conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001CAD8D1E290> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:28:38 - pika.connection - INFO - Closing connection (200): 'Normal shutdown'
2025-03-02 20:28:38 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2144, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55573, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:28:38 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2144, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55573, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:28:38 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2144, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55573, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:28:38 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=True, error-arg=None; pending-error=ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:28:38 - pika.connection - INFO - Stack terminated due to ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:28:38 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2144, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55573, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:28:38 - pika.adapters.blocking_connection - INFO - User-initiated close: result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED transport=None params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>, error=ConnectionClosedByClient: (200) 'Normal shutdown')
2025-03-02 20:28:38 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-41' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=TypeError("unhashable type: 'slice'")>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 208, in handle
    await handler(event, **values)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 56, in handle_finish_tasks
    client.publish(req,properties=pika.BasicProperties(delivery_mode=2))
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\services\rabbitmq.py", line 34, in publish
    self.channel.basic_publish(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 2259, in basic_publish
    self._impl.basic_publish(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\channel.py", line 427, in basic_publish
    self._send_method(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\channel.py", line 1415, in _send_method
    self.connection._send_method(self.channel_number, method, content)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\connection.py", line 2253, in _send_method
    self._send_message(channel_number, method, content)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\connection.py", line 2283, in _send_message
    frame_body = frame.Body(channel_number, content[1][start:end])
TypeError: unhashable type: 'slice'
2025-03-02 20:29:34 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 20:29:41 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:29:41 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:29:41 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:29:41 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:29:41 UTC], next run at: 2025-03-02 12:29:41 UTC)" (scheduled at 2025-03-02 12:29:41.149294+00:00)
2025-03-02 20:29:41 - fastapi - INFO - 开始采集数据
2025-03-02 20:29:41 - fastapi - INFO - 采集数据结束
2025-03-02 20:29:41 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:29:41 UTC], next run at: 2025-03-02 12:29:41 UTC)" executed successfully
2025-03-02 20:29:41 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:29:41 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2056, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55694, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:29:41 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001B5E352D690>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001B5E352D690> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:29:41 - pika.adapters.utils.connection_workflow - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001B5E352D690> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:29:41 - pika.adapters.utils.connection_workflow - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001B5E352D690> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:29:41 - pika.adapters.blocking_connection - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001B5E352D690> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:29:41 - pika.adapters.blocking_connection - INFO - Created channel=1
2025-03-02 20:29:41 - pika.adapters.blocking_connection - INFO - Closing connection (200): Normal shutdown
2025-03-02 20:29:41 - pika.channel - INFO - Closing channel (200): 'Normal shutdown' on <Channel number=1 OPEN conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001B5E352D690> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:29:41 - pika.channel - INFO - Received <Channel.CloseOk> on <Channel number=1 CLOSING conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000001B5E352D690> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:29:41 - pika.connection - INFO - Closing connection (200): 'Normal shutdown'
2025-03-02 20:29:41 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2056, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55694, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:29:41 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2056, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55694, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:29:41 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2056, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55694, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:29:41 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=True, error-arg=None; pending-error=ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:29:41 - pika.connection - INFO - Stack terminated due to ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:29:41 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2056, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 55694, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:29:41 - pika.adapters.blocking_connection - INFO - User-initiated close: result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED transport=None params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>, error=ConnectionClosedByClient: (200) 'Normal shutdown')
2025-03-02 20:29:41 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-13' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=TypeError('Object of type InterestMetaData is not JSON serializable')>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 208, in handle
    await handler(event, **values)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 57, in handle_finish_tasks
    client.publish(json.dumps(req),properties=pika.BasicProperties(delivery_mode=2))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type InterestMetaData is not JSON serializable
2025-03-02 20:30:41 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 20:31:09 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:31:09 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:31:09 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:31:09 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:31:09 UTC], next run at: 2025-03-02 12:31:09 UTC)" (scheduled at 2025-03-02 12:31:09.882342+00:00)
2025-03-02 20:31:10 - fastapi - INFO - 开始采集数据
2025-03-02 20:31:10 - fastapi - INFO - 采集数据结束
2025-03-02 20:31:10 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:31:09 UTC], next run at: 2025-03-02 12:31:09 UTC)" executed successfully
2025-03-02 20:31:10 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:31:10 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56274, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:31:10 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311A5600>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311A5600> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:31:10 - pika.adapters.utils.connection_workflow - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311A5600> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:31:10 - pika.adapters.utils.connection_workflow - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311A5600> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:31:10 - pika.adapters.blocking_connection - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311A5600> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:31:10 - pika.adapters.blocking_connection - INFO - Created channel=1
2025-03-02 20:31:10 - pika.adapters.blocking_connection - INFO - Closing connection (200): Normal shutdown
2025-03-02 20:31:10 - pika.channel - INFO - Closing channel (200): 'Normal shutdown' on <Channel number=1 OPEN conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311A5600> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:31:10 - pika.channel - INFO - Received <Channel.CloseOk> on <Channel number=1 CLOSING conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311A5600> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:31:10 - pika.connection - INFO - Closing connection (200): 'Normal shutdown'
2025-03-02 20:31:10 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56274, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:31:10 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56274, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:31:10 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56274, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:31:10 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=True, error-arg=None; pending-error=ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:31:10 - pika.connection - INFO - Stack terminated due to ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:31:10 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56274, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:31:10 - pika.adapters.blocking_connection - INFO - User-initiated close: result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED transport=None params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>, error=ConnectionClosedByClient: (200) 'Normal shutdown')
2025-03-02 20:31:10 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-19' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=TypeError("unhashable type: 'slice'")>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 208, in handle
    await handler(event, **values)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 57, in handle_finish_tasks
    client.publish(req,properties=pika.BasicProperties(delivery_mode=2))
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\services\rabbitmq.py", line 34, in publish
    self.channel.basic_publish(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\adapters\blocking_connection.py", line 2259, in basic_publish
    self._impl.basic_publish(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\channel.py", line 427, in basic_publish
    self._send_method(
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\channel.py", line 1415, in _send_method
    self.connection._send_method(self.channel_number, method, content)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\connection.py", line 2253, in _send_method
    self._send_message(channel_number, method, content)
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\pika\connection.py", line 2283, in _send_message
    frame_body = frame.Body(channel_number, content[1][start:end])
TypeError: unhashable type: 'slice'
2025-03-02 20:33:37 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:33:37 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:33:37 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:33:37 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:33:37 UTC], next run at: 2025-03-02 12:33:37 UTC)" (scheduled at 2025-03-02 12:33:37.578239+00:00)
2025-03-02 20:33:37 - fastapi - INFO - 开始采集数据
2025-03-02 20:33:37 - fastapi - INFO - 采集数据结束
2025-03-02 20:33:37 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:33:37 UTC], next run at: 2025-03-02 12:33:37 UTC)" executed successfully
2025-03-02 20:33:37 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:33:37 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2632, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56548, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:33:37 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311421D0>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311421D0> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:33:37 - pika.adapters.utils.connection_workflow - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311421D0> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:33:37 - pika.adapters.utils.connection_workflow - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311421D0> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:33:37 - pika.adapters.blocking_connection - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x0000017C311421D0> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:33:37 - pika.adapters.blocking_connection - INFO - Created channel=1
2025-03-02 20:34:50 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 20:35:37 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:35:37 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:35:37 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:35:37 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:35:37 UTC], next run at: 2025-03-02 12:35:37 UTC)" (scheduled at 2025-03-02 12:35:37.223814+00:00)
2025-03-02 20:35:37 - fastapi - INFO - 开始采集数据
2025-03-02 20:35:37 - fastapi - INFO - 采集数据结束
2025-03-02 20:35:37 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:35:37 UTC], next run at: 2025-03-02 12:35:37 UTC)" executed successfully
2025-03-02 20:35:37 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:35:37 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56771, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:35:37 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000002582CD61270>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000002582CD61270> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:35:37 - pika.adapters.utils.connection_workflow - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000002582CD61270> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:35:37 - pika.adapters.utils.connection_workflow - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000002582CD61270> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:35:37 - pika.adapters.blocking_connection - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000002582CD61270> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:35:37 - pika.adapters.blocking_connection - INFO - Created channel=1
2025-03-02 20:35:37 - pika.adapters.blocking_connection - INFO - Closing connection (200): Normal shutdown
2025-03-02 20:35:37 - pika.channel - INFO - Closing channel (200): 'Normal shutdown' on <Channel number=1 OPEN conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000002582CD61270> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:35:37 - pika.channel - INFO - Received <Channel.CloseOk> on <Channel number=1 CLOSING conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x000002582CD61270> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:35:37 - pika.connection - INFO - Closing connection (200): 'Normal shutdown'
2025-03-02 20:35:37 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56771, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:35:37 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56771, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:35:37 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56771, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:35:37 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=True, error-arg=None; pending-error=ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:35:37 - pika.connection - INFO - Stack terminated due to ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:35:37 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2240, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56771, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:35:37 - pika.adapters.blocking_connection - INFO - User-initiated close: result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED transport=None params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>, error=ConnectionClosedByClient: (200) 'Normal shutdown')
2025-03-02 20:35:37 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-25' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=TypeError('Object of type date is not JSON serializable')>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 208, in handle
    await handler(event, **values)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 57, in handle_finish_tasks
    client.publish(json.dumps(req),properties=pika.BasicProperties(delivery_mode=2))
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type date is not JSON serializable
2025-03-02 20:36:53 - apscheduler.scheduler - INFO - Scheduler started
2025-03-02 20:37:11 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-02 20:37:11 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-02 20:37:11 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-02 20:37:11 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-02 12:37:11 UTC], next run at: 2025-03-02 12:37:11 UTC)" (scheduled at 2025-03-02 12:37:11.406873+00:00)
2025-03-02 20:37:11 - fastapi - INFO - 开始采集数据
2025-03-02 20:37:11 - fastapi - INFO - 采集数据结束
2025-03-02 20:37:11 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-02 12:37:11 UTC], next run at: 2025-03-02 12:37:11 UTC)" executed successfully
2025-03-02 20:37:11 - pika.adapters.utils.connection_workflow - INFO - Pika version 1.3.2 connecting to ('::1', 5672, 0, 0)
2025-03-02 20:37:11 - pika.adapters.utils.io_services_utils - INFO - Socket connected: <socket.socket fd=2160, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56883, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:37:11 - pika.adapters.utils.connection_workflow - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x00000260DC535240>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x00000260DC535240> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>).
2025-03-02 20:37:11 - pika.adapters.utils.connection_workflow - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x00000260DC535240> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:37:11 - pika.adapters.utils.connection_workflow - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x00000260DC535240> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:37:11 - pika.adapters.blocking_connection - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x00000260DC535240> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>
2025-03-02 20:37:11 - pika.adapters.blocking_connection - INFO - Created channel=1
2025-03-02 20:37:11 - pika.adapters.blocking_connection - INFO - Closing connection (200): Normal shutdown
2025-03-02 20:37:11 - pika.channel - INFO - Closing channel (200): 'Normal shutdown' on <Channel number=1 OPEN conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x00000260DC535240> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:37:11 - pika.channel - INFO - Received <Channel.CloseOk> on <Channel number=1 CLOSING conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x00000260DC535240> params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>
2025-03-02 20:37:11 - pika.connection - INFO - Closing connection (200): 'Normal shutdown'
2025-03-02 20:37:11 - pika.adapters.utils.io_services_utils - INFO - Aborting transport connection: state=1; <socket.socket fd=2160, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56883, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:37:11 - pika.adapters.utils.io_services_utils - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=2160, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56883, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:37:11 - pika.adapters.utils.io_services_utils - INFO - Deactivating transport: state=1; <socket.socket fd=2160, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56883, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:37:11 - pika.connection - INFO - AMQP stack terminated, failed to connect, or aborted: opened=True, error-arg=None; pending-error=ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:37:11 - pika.connection - INFO - Stack terminated due to ConnectionClosedByClient: (200) 'Normal shutdown'
2025-03-02 20:37:11 - pika.adapters.utils.io_services_utils - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=2160, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56883, 0, 0), raddr=('::1', 5672, 0, 0)>
2025-03-02 20:37:11 - pika.adapters.blocking_connection - INFO - User-initiated close: result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED transport=None params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>, error=ConnectionClosedByClient: (200) 'Normal shutdown')
2025-03-03 14:12:03 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 14:19:05 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 14:26:10 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 14:26:10 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 14:26:10 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 14:26:10 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 06:26:10 UTC], next run at: 2025-03-03 06:26:10 UTC)" (scheduled at 2025-03-03 06:26:10.834876+00:00)
2025-03-03 14:26:10 - fastapi - INFO - 开始采集数据
2025-03-03 14:26:10 - fastapi - INFO - 采集数据结束
2025-03-03 14:26:10 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 06:26:10 UTC], next run at: 2025-03-03 06:26:10 UTC)" executed successfully
2025-03-03 14:26:11 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-157' coro=<_dispatch_as_task.<locals>.task() done, defined at c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py:51> exception=TypeError('aio_pika.message.Message() argument after ** must be a mapping, not BasicProperties')>
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\dispatcher.py", line 52, in task
    await asyncio.gather(*[handler.handle((event_name, payload)) for handler in handlers])
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\fastapi_events\handlers\local.py", line 208, in handle
    await handler(event, **values)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\handlers\historical_task_handler.py", line 57, in handle_finish_tasks
    await client.publish(json.dumps(req),properties=pika.BasicProperties(delivery_mode=2))
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\services\rabbitmq.py", line 31, in publish
    Message(body=message.encode(), **(properties or {})),
TypeError: aio_pika.message.Message() argument after ** must be a mapping, not BasicProperties
2025-03-03 14:27:50 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 14:28:01 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 14:28:01 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 14:28:01 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 14:28:01 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 06:28:01 UTC], next run at: 2025-03-03 06:28:01 UTC)" (scheduled at 2025-03-03 06:28:01.030843+00:00)
2025-03-03 14:28:01 - fastapi - INFO - 开始采集数据
2025-03-03 14:28:01 - fastapi - INFO - 采集数据结束
2025-03-03 14:28:01 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 06:28:01 UTC], next run at: 2025-03-03 06:28:01 UTC)" executed successfully
2025-03-03 14:45:03 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 14:58:13 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 14:58:13 - fastapi - INFO - 任务 2 已加入队列等待执行
2025-03-03 14:58:14 - apscheduler.scheduler - INFO - Removed job historical_2
2025-03-03 14:58:14 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 06:58:13 UTC], next run at: 2025-03-03 06:58:13 UTC)" (scheduled at 2025-03-03 06:58:13.969183+00:00)
2025-03-03 14:58:14 - fastapi - INFO - 开始采集数据
2025-03-03 14:58:24 - fastapi - INFO - Data fetch successful for API function: interest_by_region, with parameters - Keywords: ['deepseek', 'deepsook'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-03 14:58:25 - fastapi - INFO - 采集数据结束
2025-03-03 14:58:25 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 06:58:13 UTC], next run at: 2025-03-03 06:58:13 UTC)" executed successfully
2025-03-03 15:00:59 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 15:00:59 - fastapi - INFO - 任务 3 已加入队列等待执行
2025-03-03 15:00:59 - apscheduler.scheduler - INFO - Removed job historical_3
2025-03-03 15:00:59 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 07:00:59 UTC], next run at: 2025-03-03 07:00:59 UTC)" (scheduled at 2025-03-03 07:00:59.343203+00:00)
2025-03-03 15:00:59 - fastapi - INFO - 开始采集数据
2025-03-03 15:01:15 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-03-03 15:02:32 - fastapi - WARNING - Rate limited. Retrying after 60 seconds...
2025-03-03 15:03:33 - fastapi - INFO - Data fetch successful for API function: interest_by_region, with parameters - Keywords: ['latin', 'english'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-03 15:04:01 - fastapi - INFO - 采集数据结束
2025-03-03 15:04:01 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 07:00:59 UTC], next run at: 2025-03-03 07:00:59 UTC)" executed successfully
2025-03-03 16:18:25 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001B200B6FD90>])])
2025-03-03 16:18:25 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:29:42 - fastapi - ERROR - Failed to process message: (builtins.TypeError) SQLite DateTime type only accepts Python datetime and date objects as input.
[SQL: INSERT INTO scheduled_tasks (job_type, keywords, geo_code, duration, start_date, interval, enabled) VALUES (?, ?, ?, ?, ?, ?, ?)]
[parameters: [{'start_date': '2025-01-01', 'job_type': 'time', 'keywords': ['ok', 'Okay'], 'interval': '1d', 'duration': 2, 'geo_code': ''}]]
2025-03-03 16:29:42 - fastapi - ERROR - Failed to process message: <ContextVar name='fastapi_middleware_identifier' at 0x000001B200B5B1A0>
2025-03-03 16:34:07 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001C8A0FEBE20>])])
2025-03-03 16:34:07 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:34:22 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 16:34:22 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 16:34:22 - fastapi - ERROR - Failed to process message: <ContextVar name='fastapi_middleware_identifier' at 0x000001C8A0FE29D0>
2025-03-03 16:36:30 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000002564BA87E20>])])
2025-03-03 16:36:30 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:36:58 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 16:36:58 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 16:36:59 - fastapi - ERROR - Failed to process message: <ContextVar name='fastapi_middleware_identifier' at 0x000002564BA7E9D0>
2025-03-03 16:39:15 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 16:39:15 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 16:40:31 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001DA172E7E20>])])
2025-03-03 16:40:31 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:42:22 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001FE448B5A20>])])
2025-03-03 16:42:22 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:42:38 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 16:42:38 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 16:42:38 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 16:42:38 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 16:42:38 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 16:42:38 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 08:42:38 UTC], next run at: 2025-03-03 08:42:38 UTC)" (scheduled at 2025-03-03 08:42:38.156817+00:00)
2025-03-03 16:42:38 - fastapi - INFO - 开始采集数据
2025-03-03 16:42:38 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-03-03 08:42:38 UTC], next run at: 2025-03-03 08:42:38 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 181, in run_coroutine_job
    retval = await job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 57, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 13, in execute_task
    last_id= get_interest_by_region(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 54, in get_interest_by_region
    timeframe_start = datetime.strptime(start, "%Y-%m-%d").date()
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 568, in _strptime_datetime
    tt, fraction, gmtoff_fraction = _strptime(data_string, format)
  File "C:\Users\a2450\scoop\apps\python310\3.10.11\lib\_strptime.py", line 352, in _strptime
    raise ValueError("unconverted data remains: %s" %
ValueError: unconverted data remains:  00:00:00
2025-03-03 16:48:52 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x00000299E9587E20>])])
2025-03-03 16:48:52 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:49:17 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001CC70DD9B40>])])
2025-03-03 16:49:17 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:53:06 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000023946231AB0>])])
2025-03-03 16:53:06 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:53:16 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 16:53:16 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 16:53:16 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 16:53:16 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 08:53:16 UTC], next run at: 2025-03-03 08:53:16 UTC)" (scheduled at 2025-03-03 08:53:16.045430+00:00)
2025-03-03 16:53:16 - fastapi - INFO - 开始采集数据
2025-03-03 16:53:38 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-03-03 08:53:16 UTC], next run at: 2025-03-03 08:53:16 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 181, in run_coroutine_job
    retval = await job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 57, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 13, in execute_task
    last_id= get_interest_by_region(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 54, in get_interest_by_region
    timeframe_start = datetime.strptime(start, "%Y-%m-%d").date()
TypeError: strptime() argument 1 must be str, not datetime.date
2025-03-03 16:54:13 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 16:54:13 - fastapi - INFO - 任务 2 已加入队列等待执行
2025-03-03 16:54:13 - apscheduler.scheduler - INFO - Removed job historical_2
2025-03-03 16:54:13 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 08:54:13 UTC], next run at: 2025-03-03 08:54:13 UTC)" (scheduled at 2025-03-03 08:54:13.492585+00:00)
2025-03-03 16:54:13 - fastapi - INFO - 开始采集数据
2025-03-03 16:54:54 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000020B882F16C0>])])
2025-03-03 16:54:54 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:54:57 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 16:54:57 - fastapi - INFO - 任务 3 已加入队列等待执行
2025-03-03 16:54:57 - apscheduler.scheduler - INFO - Removed job historical_3
2025-03-03 16:54:57 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 08:54:57 UTC], next run at: 2025-03-03 08:54:57 UTC)" (scheduled at 2025-03-03 08:54:57.523336+00:00)
2025-03-03 16:54:57 - fastapi - INFO - 开始采集数据
2025-03-03 16:55:13 - fastapi - INFO - Data fetch successful for API function: interest_by_region, with parameters - Keywords: ['latin', 'english'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-03 16:55:13 - fastapi - INFO - 采集数据结束
2025-03-03 16:55:13 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 08:54:57 UTC], next run at: 2025-03-03 08:54:57 UTC)" executed successfully
2025-03-03 16:55:21 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 16:55:21 - fastapi - INFO - 任务 4 已加入队列等待执行
2025-03-03 16:55:21 - apscheduler.scheduler - INFO - Removed job historical_4
2025-03-03 16:55:21 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 08:55:21 UTC], next run at: 2025-03-03 08:55:21 UTC)" (scheduled at 2025-03-03 08:55:21.252663+00:00)
2025-03-03 16:55:21 - fastapi - INFO - 开始采集数据
2025-03-03 16:55:39 - apscheduler.executors.default - ERROR - Job "execute_historical_task (trigger: date[2025-03-03 08:55:21 UTC], next run at: 2025-03-03 08:55:21 UTC)" raised an exception
Traceback (most recent call last):
  File "c:\Users\a2450\Desktop\project\backend\trendsmicro\.venv\lib\site-packages\apscheduler\executors\base.py", line 181, in run_coroutine_job
    retval = await job.func(*job.args, **job.kwargs)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 57, in execute_historical_task
    raise e  # 重新抛出异常以便APScheduler记录日志
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 33, in execute_historical_task
    interest_id=execute_task(
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\jobs.py", line 15, in execute_task
    last_id= get_interest_over_time(keywords, geo_code, interval, start_date, end_date,task_id)
  File "C:\Users\a2450\Desktop\project\backend\trendsmicro\microservices\collector\src\core\trends.py", line 127, in get_interest_over_time
    timeframe_start = datetime.strptime(start, "%Y-%m-%d").date()
TypeError: strptime() argument 1 must be str, not datetime.date
2025-03-03 16:56:21 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000022AE6AE5AB0>])])
2025-03-03 16:56:21 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:58:40 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000019F97A79870>])])
2025-03-03 16:58:40 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:59:07 - fastapi - ERROR - Failed to process message: api.models.tasks.ScheduledTask() argument after ** must be a mapping, not ScheduledTaskRequest
2025-03-03 16:59:38 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000002337E7BD870>])])
2025-03-03 16:59:38 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 16:59:43 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 16:59:43 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 16:59:45 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 16:59:45 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 16:59:45 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 16:59:45 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 08:59:45 UTC], next run at: 2025-03-03 08:59:45 UTC)" (scheduled at 2025-03-03 08:59:45.850219+00:00)
2025-03-03 16:59:45 - fastapi - INFO - 开始采集数据
2025-03-03 17:00:39 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x00000224C7E19AB0>])])
2025-03-03 17:00:40 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 17:00:41 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 17:00:41 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 17:00:41 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 17:00:41 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 17:00:41 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 17:00:41 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 09:00:41 UTC], next run at: 2025-03-03 09:00:41 UTC)" (scheduled at 2025-03-03 09:00:41.869638+00:00)
2025-03-03 17:00:41 - fastapi - INFO - 开始采集数据
2025-03-03 17:01:23 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x00000177F2021AB0>])])
2025-03-03 17:01:23 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 17:01:30 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 17:01:30 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 17:01:30 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 17:01:30 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 17:01:30 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 17:01:30 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 09:01:30 UTC], next run at: 2025-03-03 09:01:30 UTC)" (scheduled at 2025-03-03 09:01:30.785964+00:00)
2025-03-03 17:01:30 - fastapi - INFO - 开始采集数据
2025-03-03 17:01:43 - fastapi - INFO - Data fetch successful for API function: interest_by_region, with parameters - Keywords: ['ok', 'Okay'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-03 17:01:43 - fastapi - INFO - 采集数据结束
2025-03-03 17:01:43 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 09:01:30 UTC], next run at: 2025-03-03 09:01:30 UTC)" executed successfully
2025-03-03 18:45:32 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000002E5F9751AB0>])])
2025-03-03 18:45:32 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 18:47:02 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 18:47:02 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 18:47:02 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 18:47:02 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 18:47:02 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 18:47:02 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 10:47:02 UTC], next run at: 2025-03-03 10:47:02 UTC)" (scheduled at 2025-03-03 10:47:02.410393+00:00)
2025-03-03 18:47:02 - fastapi - INFO - 开始采集数据
2025-03-03 18:47:13 - fastapi - INFO - Data fetch successful for API function: interest_by_region, with parameters - Keywords: ['ok', 'Okay'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-03 18:47:13 - fastapi - INFO - 采集数据结束
2025-03-03 18:47:13 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 10:47:02 UTC], next run at: 2025-03-03 10:47:02 UTC)" executed successfully
2025-03-03 18:54:12 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001E3B13C5A20>])])
2025-03-03 18:54:12 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 18:54:35 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 18:54:35 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 18:54:35 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 18:54:35 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 18:58:53 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000029E68B6E050>])])
2025-03-03 18:58:53 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 19:00:24 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 19:00:24 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 19:00:24 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 19:00:24 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 19:00:39 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 19:00:39 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 19:00:39 CST], next run at: 2025-03-03 19:00:39 CST)" (scheduled at 2025-03-03 19:00:39.855318+08:00)
2025-03-03 19:00:39 - fastapi - INFO - 开始采集数据
2025-03-03 19:00:50 - fastapi - INFO - Data fetch successful for API function: interest_by_region, with parameters - Keywords: ['ok', 'Okay'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-03 19:00:50 - fastapi - INFO - 采集数据结束
2025-03-03 19:00:50 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 19:00:39 CST], next run at: 2025-03-03 19:00:39 CST)" executed successfully
2025-03-03 19:03:06 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000017A52A960E0>])])
2025-03-03 19:03:07 - apscheduler.scheduler - INFO - Scheduler started
2025-03-03 19:03:49 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-03 19:03:49 - fastapi - INFO - interval_config: {'days': 1}
2025-03-03 19:03:49 - apscheduler.scheduler - INFO - Added job "execute_historical_task" to job store "default"
2025-03-03 19:03:49 - fastapi - INFO - 任务 1 已加入队列等待执行
2025-03-03 19:04:04 - apscheduler.scheduler - INFO - Removed job historical_1
2025-03-03 19:04:04 - apscheduler.executors.default - INFO - Running job "execute_historical_task (trigger: date[2025-03-03 19:04:04 CST], next run at: 2025-03-03 19:04:04 CST)" (scheduled at 2025-03-03 19:04:04.265266+08:00)
2025-03-03 19:04:04 - fastapi - INFO - 开始采集数据
2025-03-03 19:04:14 - fastapi - INFO - Data fetch successful for API function: interest_by_region, with parameters - Keywords: ['ok', 'Okay'], Timeframe: 2025-01-01 2025-02-28, Geo: 
2025-03-03 19:04:14 - fastapi - INFO - 采集数据结束
2025-03-03 19:04:14 - apscheduler.executors.default - INFO - Job "execute_historical_task (trigger: date[2025-03-03 19:04:04 CST], next run at: 2025-03-03 19:04:04 CST)" executed successfully
2025-03-04 20:46:58 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x00000237F6456050>])])
2025-03-04 20:46:58 - apscheduler.scheduler - INFO - Scheduler started
2025-03-04 20:47:04 - apscheduler.scheduler - INFO - Removed job scheduled_1
2025-03-04 20:47:04 - apscheduler.executors.default - WARNING - Run time of job "execute_scheduled_task (trigger: interval[1 day, 0:00:00], next run at: 2025-03-04 19:03:49 CST)" was missed by 1:43:15.613814
2025-03-05 20:12:22 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000002759EE5A290>])])
2025-03-05 20:12:23 - apscheduler.scheduler - INFO - Scheduler started
2025-03-06 12:48:02 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000015418716290>])])
2025-03-06 12:48:03 - apscheduler.scheduler - INFO - Scheduler started
2025-03-08 13:20:33 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000029AA96CA290>])])
2025-03-08 13:20:37 - aio_pika.robust_connection - INFO - Connection to amqp://admin:******@localhost:5672// closed. Reconnecting after 5 seconds.
2025-03-09 14:33:28 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001C6C8DFA290>])])
2025-03-09 14:33:28 - apscheduler.scheduler - INFO - Scheduler started
2025-03-10 15:35:59 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x00000217B5646290>])])
2025-03-10 15:35:59 - apscheduler.scheduler - INFO - Scheduler started
2025-03-12 15:10:13 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001DDC9CE3760>])])
2025-03-12 15:10:13 - apscheduler.scheduler - INFO - Scheduler started
2025-03-13 10:13:14 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000016A4A91A290>])])
2025-03-13 10:13:14 - apscheduler.scheduler - INFO - Scheduler started
2025-03-13 15:12:50 - apscheduler.scheduler - INFO - Added job "execute_scheduled_task" to job store "default"
2025-03-13 15:12:50 - fastapi - INFO - interval_config: {'days': 1}
2025-03-13 18:00:29 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000002D612586290>])])
2025-03-13 18:00:29 - apscheduler.scheduler - INFO - Scheduler started
2025-03-13 18:08:04 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000001FDECBB5CF0>])])
2025-03-13 18:08:04 - apscheduler.scheduler - INFO - Scheduler started
2025-03-13 18:08:20 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000025336AF6560>])])
2025-03-13 18:08:20 - apscheduler.scheduler - INFO - Scheduler started
2025-03-13 18:09:54 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000024584C59CF0>])])
2025-03-13 18:09:54 - apscheduler.scheduler - INFO - Scheduler started
2025-03-13 18:12:31 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x0000020FBBCBDCF0>])])
2025-03-13 18:12:31 - apscheduler.scheduler - INFO - Scheduler started
2025-03-13 18:14:49 - aio_pika - INFO - dict_items([('collector_task_request', [<function subject_task_request at 0x000002016B338550>])])
2025-03-13 18:14:49 - apscheduler.scheduler - INFO - Scheduler started
